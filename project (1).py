# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zAZYp8_TjIzJwpYJDMdE401bsCFLcEdg

# Upload the dataset
"""

from google.colab import files
uploaded = files.upload()

"""# Load the dataset"""

import pandas as pd

# Load your churn dataset
df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# View the first few rows
df.head()

"""# Data Exploration"""

# Dataset shape
print("Shape:", df.shape)

# Column names
print("Columns:", df.columns.tolist())

# Check data types and missing values
df.info()
df.isnull().sum()

# Summary statistics
df.describe()

"""# Check for Missing Values and Duplicates"""

# Load the dataset
import pandas as pd

df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# Check for missing values
print("üîç Missing Values in Each Column:")
print(df.isnull().sum())

# Check for duplicate rows
print("\nüìã Number of Duplicate Rows:", df.duplicated().sum())

"""# Visualize a Few Features"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# Check column names (just to confirm)
print("Columns in dataset:", df.columns.tolist())

# Visualization 1: Histogram of Unemployment Rate
if 'Unemployment Rate' in df.columns:
    sns.histplot(df['Unemployment Rate'], kde=True, color='skyblue')
    plt.title('Distribution of Unemployment Rate')
    plt.xlabel('Unemployment Rate')
    plt.ylabel('Frequency')
    plt.show()

# Visualization 2: Boxplot - Crime Rate vs Churn
if 'Crime Rate' in df.columns and 'churn' in df.columns:
    sns.boxplot(x='churn', y='Crime Rate', data=df, palette='Set2')
    plt.title('Crime Rate by Churn')
    plt.xlabel('Churn')
    plt.ylabel('Crime Rate')
    plt.show()

# Visualization 3: Correlation Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Visualization 4: Pairplot (Optional: Only for small feature sets)
selected_cols = ['Unemployment Rate', 'Poverty Rate', 'Crime Rate', 'churn']
available_cols = [col for col in selected_cols if col in df.columns]

if len(available_cols) >= 2:
    sns.pairplot(df[available_cols], hue='churn')
    plt.suptitle("Pairwise Feature Relationships", y=1.02)
    plt.show()

"""# Identify Target and Features"""

print("üìã Column Names:", df.columns.tolist())

import pandas as pd

# Load the dataset
df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# Show available columns
print("üìã Columns in Dataset:")
print(df.columns.tolist())

# Set the target column (change if your target column has a different name)
target = 'churn'

# Check if the target exists
if target in df.columns:
    features = df.columns.drop(target)
    print("\nüéØ Target Column:", target)
    print("\n‚úÖ Feature Columns:")
    print(features.tolist())
else:
    print(f"\n‚ö†Ô∏è Target column '{target}' not found in dataset. Please check the column names.")

target = 'churn'

"""# Convert Categorical Columns to Numerical"""

import pandas as pd

# Load the dataset
df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# Identify categorical columns (type 'object')
categorical_cols = df.select_dtypes(include=['object']).columns

print("üî§ Categorical Columns:")
print(categorical_cols.tolist())

# Convert categorical variables to dummy/indicator variables
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print("\n‚úÖ Dataset after Encoding:")
print(df_encoded.head())

"""
# One-Hot Encoding"""

import pandas as pd

# Load your dataset
df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# Identify categorical columns (usually of type 'object')
categorical_cols = df.select_dtypes(include=['object']).columns

print("üî§ Categorical Columns:")
print(categorical_cols.tolist())

# Perform one-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print("\n‚úÖ Dataset after One-Hot Encoding:")
print(df_encoded.head())

"""
# Feature Scaling"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Step 1: Load the dataset
df = pd.read_csv('crime_vs_socioeconomic_factors.csv')

# Step 2: Identify and encode categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Step 3: Set target column (replace 'churn' if your dataset uses another name)
target = 'churn'

if target in df_encoded.columns:
    # Step 4: Separate features and target
    X = df_encoded.drop(columns=[target])
    y = df_encoded[target]

    # Step 5: Feature scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print("‚úÖ Feature scaling completed successfully.")
    print("üìê Scaled feature shape:", X_scaled.shape)
else:
    print(f"‚ö†Ô∏è Target column '{target}' not found. Please check your column names.")
    print("Available columns:", df_encoded.columns.tolist())

"""
# Train-Test Split"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('/content/crime_vs_socioeconomic_factors.csv')

# Check for object (categorical) columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Convert categorical columns to numerical using one-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Set the target variable (update 'churn' if your column name is different)
target = 'churn'

# Ensure the target column exists
if target in df_encoded.columns:
    # Separate features and target
    X = df_encoded.drop(columns=[target])
    y = df_encoded[target]

    # Feature Scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Train-Test Split (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )

    # Print the result
    print("‚úÖ Train-Test Split Completed Successfully")
    print("Training Features Shape:", X_train.shape)
    print("Testing Features Shape:", X_test.shape)
else:
    print(f"‚ùå Error: Target column '{target}' not found in the dataset.")
    print("Available columns are:", df_encoded.columns.tolist())

"""# Model Building"""

# prompt: model building give me the code without error

import pandas as pd
# Import necessary libraries for model building
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset (assuming it's already uploaded and available at the specified path)
# df = pd.read_csv('/content/crime_vs_socioeconomic_factors.csv') # Assuming the file is already loaded and processed

# The code preceding this point has already loaded, encoded, scaled, and split the data
# X_train, X_test, y_train, y_test should be available from the previous steps.

# Check if the data splits are available
if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():
    # Initialize the model (using Logistic Regression as an example)
    model = LogisticRegression(random_state=42)

    # Train the model
    print("üí™ Training the model...")
    model.fit(X_train, y_train)
    print("‚úÖ Model training completed.")

    # Make predictions
    print(" predicting on the test set...")
    y_pred = model.predict(X_test)
    print("‚úÖ Prediction completed.")

    # Evaluate the model
    print("\nüìä Model Evaluation:")
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
else:
    print("‚ùå Error: Data splits (X_train, X_test, y_train, y_test) not found.")
    print("Please ensure the previous steps for data loading, encoding, scaling, and splitting were executed successfully.")

"""# Evaluation"""

# prompt: give me the code without error

import matplotlib.pyplot as plt
# Evaluate the model (assuming y_test and y_pred are available from the previous step)

# Check if prediction results are available
if 'y_test' in locals() and 'y_pred' in locals():
    print("\nüìä Model Evaluation:")

    # Calculate Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    # Print Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Print Confusion Matrix
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # Visualize the Confusion Matrix (Optional but recommended)
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

else:
    print("‚ùå Error: Prediction results (y_test or y_pred) not found.")
    print("Please ensure the 'Model Building' step was executed successfully to generate predictions.")

"""# Make Predictions from New **Input**"""

# prompt: Make Predictions from New Input give me without error

import pandas as pd
# Example of making predictions with new input data
# You need to preprocess the new input data in the same way as the training data

# Assume you have new data in a pandas DataFrame called `new_data_df`
# This new data should have the same columns as your original features (before encoding and scaling)
# Let's create a dummy new data sample for demonstration.
# Replace this with your actual new input data.
new_data = {
    'Unemployment Rate': [5.5],
    'Poverty Rate': [12.1],
    'Crime Rate': [350.5],
    # ... include all feature columns present in your original data *before* encoding
    # Make sure to include the columns that were categorical in the original data,
    # even if the new data has numerical values for them after encoding.
    # For demonstration, let's assume 'State' was a categorical column.
    'State': ['California']
}
new_data_df = pd.DataFrame(new_data)

print("Original New Input Data:")
print(new_data_df)

# Apply the same preprocessing steps as the training data:
# 1. One-Hot Encoding (using the same columns as the training data)
# We need to make sure the columns in the new data after encoding match the training data.
# The simplest way is to re-apply get_dummies and then align the columns.

# First, get the list of all columns from the training data *before* scaling
# This assumes `X` from the 'Train-Test Split' step is still available and represents
# the features *after* encoding but *before* scaling.
if 'X' in locals():
    training_columns_after_encoding = X.columns.tolist()

    # Encode the new data
    new_data_encoded = pd.get_dummies(new_data_df, columns=categorical_cols, drop_first=True)

    # Align columns - add missing columns (filled with 0) that were in the training data
    # but not in the new data after encoding, and drop extra columns.
    new_data_aligned = new_data_encoded.reindex(columns=training_columns_after_encoding, fill_value=0)

    print("\nNew Input Data after One-Hot Encoding and Alignment:")
    print(new_data_aligned)

    # 2. Feature Scaling (using the *same* scaler fitted on the training data)
    # Ensure the `scaler` object from the 'Feature Scaling' step is available.
    if 'scaler' in locals():
        new_data_scaled = scaler.transform(new_data_aligned)

        print("\nNew Input Data after Scaling:")
        print(new_data_scaled)
        print("Scaled shape:", new_data_scaled.shape)

        # 3. Make Prediction
        # Ensure the trained `model` object from the 'Model Building' step is available.
        if 'model' in locals():
            # The model expects input in the same format as X_train (scaled features)
            prediction = model.predict(new_data_scaled)
            prediction_proba = model.predict_proba(new_data_scaled) # Get probabilities

            print("\n‚úÖ Prediction for the new input:")
            print(f"Predicted Class (0: No Churn, 1: Churn): {prediction[0]}")
            print(f"Prediction Probability (0: No Churn, 1: Churn): {prediction_proba[0]}")

            # Interpret the result
            if prediction[0] == 1:
                print("\nInterpretation: The model predicts that this new input likely corresponds to churn.")
            else:
                print("\nInterpretation: The model predicts that this new input likely corresponds to no churn.")

        else:
            print("\n‚ùå Error: Model not found. Please run the 'Model Building' step first.")
    else:
        print("\n‚ùå Error: Scaler not found. Please run the 'Feature Scaling' step first.")
else:
    print("\n‚ùå Error: Training features (X) not found. Please run the 'Train-Test Split' step first.")
    print("Ensure that X (features before scaling) is available after your data splitting step.")

"""# Convert to DataFrame and Encode"""

# prompt: Convert to DataFrame and Encode

import pandas as pd
# # Convert to DataFrame and Encode

# Load the dataset (assuming it's already uploaded and available at the specified path)
# If you already loaded it in previous steps, you might not need this line again:
# df = pd.read_csv('/content/crime_vs_socioeconomic_factors.csv')

# Check if df is already loaded
if 'df' in locals():
    print("‚úÖ Dataset DataFrame `df` is already loaded and available.")
else:
    print("‚ö†Ô∏è Dataset DataFrame `df` not found. Please ensure the file is loaded in a previous step.")
    # If the file is not loaded, load it now.
    try:
        df = pd.read_csv('/content/crime_vs_socioeconomic_factors.csv')
        print("‚úÖ Dataset loaded from /content/crime_vs_socioeconomic_factors.csv")
    except FileNotFoundError:
        print("‚ùå Error: crime_vs_socioeconomic_factors.csv not found.")
        print("Please upload the file or check the file path.")
        # Exit or handle the error appropriately if the file isn't found

# Convert DataFrame to JSON string (optional step, depends on your need)
# For example, if you want to store the DataFrame structure or a sample in JSON
# df_json = df.to_json(orient='records', lines=True)
# print("\n DataFrame as JSON (first 5 rows):")
# print(df_json[:500]) # Print only a part to avoid flooding the output

# Identify categorical columns (type 'object')
categorical_cols = df.select_dtypes(include=['object']).columns

print("\nüî§ Identified Categorical Columns for Encoding:")
print(categorical_cols.tolist())

# Convert categorical variables to dummy/indicator variables using one-hot encoding
# `drop_first=True` prevents multicollinearity by dropping the first category of each feature
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print("\n‚úÖ Dataset after One-Hot Encoding:")
print(df_encoded.head())
print("\nEncoded DataFrame Shape:", df_encoded.shape)
print("\nEncoded DataFrame Columns:", df_encoded.columns.tolist())

"""# Predict the Final Grade"""

print("\nBased on the provided code context, a Logistic Regression model for 'churn' prediction was built.")
print("Predicting 'Final Grade' (which implies a regression task) is not directly supported by the preceding code.")
print("If 'Final Grade' is a column in your dataset and you want to build a regression model,")
print("please adjust the target variable and choose a regression model.")

"""# Deployment-Building an Interactive App"""

!pip install gradio

# prompt: Define a function that does preprocessing + prediction

import pandas as pd
def preprocess_and_predict(new_data_df, model, scaler, training_columns_after_encoding, categorical_cols):
    """
    Preprocesses new input data and makes a prediction using a trained model.

    Args:
        new_data_df (pd.DataFrame): DataFrame containing the new input data.
                                     Should have the same columns as the original
                                     data *before* encoding and scaling.
        model: The trained model object (e.g., LogisticRegression).
        scaler: The fitted StandardScaler object from training.
        training_columns_after_encoding (list): List of column names from the
                                                training data after one-hot encoding
                                                but before scaling.
        categorical_cols (list): List of column names that were identified as
                                 categorical in the original data.

    Returns:
        tuple: A tuple containing:
               - prediction (numpy.ndarray): The predicted class label(s).
               - prediction_proba (numpy.ndarray): The prediction probability(ies).
               - interpretation (str): A human-readable interpretation of the prediction.
    """
    # 1. One-Hot Encoding
    # Encode the new data
    new_data_encoded = pd.get_dummies(new_data_df, columns=categorical_cols, drop_first=True)

    # Align columns - add missing columns (filled with 0) that were in the training data
    # but not in the new data after encoding, and drop extra columns.
    new_data_aligned = new_data_encoded.reindex(columns=training_columns_after_encoding, fill_value=0)

    # 2. Feature Scaling
    new_data_scaled = scaler.transform(new_data_aligned)

    # 3. Make Prediction
    prediction = model.predict(new_data_scaled)
    prediction_proba = model.predict_proba(new_data_scaled) # Get probabilities

    # 4. Interpret the result
    if prediction[0] == 1:
        interpretation = "The model predicts that this input likely corresponds to churn."
    else:
        interpretation = "The model predicts that this input likely corresponds to no churn."

    return prediction, prediction_proba, interpretation

# Example Usage (assuming `model`, `scaler`, `X`, and `categorical_cols` are available
# from the preceding code execution):

# Define a sample new data point as a dictionary or list of dictionaries
# Make sure this dictionary contains all original columns *before* encoding
sample_new_input = {
    'Unemployment Rate': [6.0],
    'Poverty Rate': [15.5],
    'Crime Rate': [400.0],
    # Include other original columns, even if they were categorical
    'State': ['New York'],
    # Add other columns from your original dataframe if applicable
    # e.g., 'Income Level': ['Medium'], 'Education': ['High School']
    # For this example, assuming only Unemployment Rate, Poverty Rate, Crime Rate, and State
    # were in the original feature set that fed into X.
}
new_input_df = pd.DataFrame(sample_new_input)

# Ensure `training_columns_after_encoding` is defined from the previous steps
# It should be the columns of your features (X) *after* one-hot encoding and *before* scaling
if 'X' in locals():
    training_columns_after_encoding = X.columns.tolist()

    # Ensure `categorical_cols` is defined from the previous steps
    if 'categorical_cols' in locals() and 'model' in locals() and 'scaler' in locals():
        # Call the preprocessing and prediction function
        predicted_class, predicted_prob, result_interpretation = preprocess_and_predict(
            new_input_df,
            model,
            scaler,
            training_columns_after_encoding,
            categorical_cols
        )

        print("\n--- Prediction using the function ---")
        print(f"New Input Data:\n{new_input_df}")
        print(f"\nPredicted Class: {predicted_class[0]}")
        print(f"Predicted Probability (0: No Churn, 1: Churn): {predicted_prob[0]}")
        print(f"Interpretation: {result_interpretation}")
    else:
        print("\n‚ùå Error: Required objects (model, scaler, categorical_cols) not found.")
        print("Please ensure the previous steps for model training and scaling were executed.")
else:
    print("\n‚ùå Error: Training features (X) not found.")
    print("Please ensure the 'Train-Test Split' step was executed successfully to define X.")

# prompt:  Preprocess the tweet

import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk

# Download necessary NLTK data if not already downloaded
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')

try:
    PorterStemmer()
except LookupError:
    nltk.download('rslp') # Required for PorterStemmer in some cases


def preprocess_tweet(tweet):
    """
    Preprocesses a single tweet by performing the following steps:
    1. Convert to lowercase.
    2. Remove URLs.
    3. Remove mentions (@...).
    4. Remove hashtags (#...).
    5. Remove punctuation.
    6. Remove digits.
    7. Remove extra whitespace.
    8. Remove stopwords.
    9. Apply stemming.

    Args:
        tweet (str): The raw tweet string.

    Returns:
        str: The preprocessed tweet string.
    """
    # 1. Convert to lowercase
    tweet = tweet.lower()

    # 2. Remove URLs
    tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet, flags=re.MULTILINE)

    # 3. Remove mentions (@...)
    tweet = re.sub(r'@\w+', '', tweet)

    # 4. Remove hashtags (#...) - keeping the text
    tweet = re.sub(r'#', '', tweet)

    # 5. Remove punctuation
    tweet = tweet.translate(str.maketrans('', '', string.punctuation))

    # 6. Remove digits
    tweet = re.sub(r'\d+', '', tweet)

    # 7. Remove extra whitespace
    tweet = re.sub(r'\s+', ' ', tweet).strip()

    # 8. Remove stopwords
    stop_words = set(stopwords.words('english'))
    word_tokens = tweet.split()
    filtered_tweet = [word for word in word_tokens if word not in stop_words]
    tweet = ' '.join(filtered_tweet)

    # 9. Apply stemming
    stemmer = PorterStemmer()
    stemmed_tweet = [stemmer.stem(word) for word in tweet.split()]
    tweet = ' '.join(stemmed_tweet)

    return tweet

# Example Usage:
sample_tweet = "This is a #sample tweet with a URL http://example.com, a mention @user, and some numbers 123!"
preprocessed_tweet = preprocess_tweet(sample_tweet)

print("Original Tweet:", sample_tweet)
print("Preprocessed Tweet:", preprocessed_tweet)

# If your dataset has a 'tweet' or 'text' column, you can apply this function to it:
# Assuming your DataFrame is `df` and the text column is named 'tweet_text'
# if 'tweet_text' in df.columns:
#     df['preprocessed_tweet'] = df['tweet_text'].apply(preprocess_tweet)
#     print("\nDataFrame with preprocessed tweets:")
#     print(df[['tweet_text', 'preprocessed_tweet']].head())
# else:
#     print("\n‚ö†Ô∏è No 'tweet_text' column found in the DataFrame to preprocess.")

# prompt:  Convert to TF-IDF

from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming your text data is in a column named 'preprocessed_tweet'
# If not, replace 'preprocessed_tweet' with the correct column name containing your text data
text_column = 'preprocessed_tweet' # Replace if needed

if text_column in df.columns:
    # Initialize the TF-IDF Vectorizer
    # You can adjust parameters like max_features, min_df, max_df, ngram_range
    tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Example: limit to top 5000 terms

    # Fit and transform the preprocessed text data
    tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])

    # The tfidf_matrix is a sparse matrix (efficient for large text data)
    print("\n‚úÖ TF-IDF conversion completed.")
    print("Shape of TF-IDF matrix:", tfidf_matrix.shape) # (Number of documents, Number of unique terms/features)

    # You can get the feature names (words)
    tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
    print("Example TF-IDF feature names (first 10):", tfidf_feature_names[:10])

    # You can convert the sparse matrix to a dense NumPy array if needed (be cautious with large datasets)
    # tfidf_dense = tfidf_matrix.todense()
    # print("Shape of dense TF-IDF matrix:", tfidf_dense.shape)

    # If you need to integrate TF-IDF features with other features (like socioeconomic factors),
    # you would concatenate the tfidf_matrix with your other scaled numerical features.
    # Example (assuming X_scaled contains your numerical/encoded features before TF-IDF):
    # from scipy.sparse import hstack
    # X_combined = hstack((X_scaled, tfidf_matrix))
    # print("Shape of combined features (Numerical + TF-IDF):", X_combined.shape)

else:
    print(f"\n‚ùå Error: Text column '{text_column}' not found in the DataFrame.")
    print("Please ensure you have a column containing your text data after preprocessing.")
    print("Available columns:", df.columns.tolist())

# prompt: Predict the emotion

# Based on the preceding code, a model for churn prediction was built.
# There is no model trained specifically for emotion prediction in the provided code.
# To predict emotion, you would need an emotion dataset and train a separate model.

print("Based on the provided code, there is no model trained for emotion prediction.")
print("The existing code is for predicting churn based on socioeconomic factors and crime rate,")
print("and includes text preprocessing utilities.")
print("To predict emotion, you would need to load an emotion dataset, train a model on it,")
print("and then use that trained model for prediction.")

# prompt:  Define the input and output

import pandas as pd
# Define the input interface for the Gradio app
# Based on the 'Make Predictions from New Input' section, the model
# expects input corresponding to the features used for training.
# These are likely numerical/encoded features like 'Unemployment Rate',
# 'Poverty Rate', 'Crime Rate', and possibly one-hot encoded versions
# of categorical features like 'State'.

# We need to list the input components that match the expected features
# of the model *after* one-hot encoding and *before* scaling, because
# our `preprocess_and_predict` function handles the encoding and scaling
# internally.

# Let's define inputs based on the example `sample_new_input`
# and assuming 'Unemployment Rate', 'Poverty Rate', 'Crime Rate' are numerical
# and 'State' is categorical.
# If you have more features in your original dataset, add them here.

# NOTE: This assumes you know the expected feature names and types
# from your original dataset before any preprocessing.
# The `categorical_cols` variable should hold the names of columns
# that were originally categorical (e.g., ['State']).

# Example Input Definition (adjust according to your actual features)
# Assuming the original features were:
# 'Unemployment Rate' (numerical)
# 'Poverty Rate' (numerical)
# 'Crime Rate' (numerical)
# 'State' (categorical - we need a dropdown or textbox for the state name)

import gradio as gr

# Assuming 'categorical_cols' list is available from previous steps
# and contains names like ['State', 'SomeOtherCategory']
# Let's get unique values for categorical columns if `df` is available
# This is useful for creating dropdowns.
categorical_inputs = {}
if 'df' in locals() and 'categorical_cols' in locals() and categorical_cols.size > 0:
    print("\nAttempting to create Gradio inputs for categorical columns...")
    for col in categorical_cols:
        if col in df.columns:
            # Get unique values, handle potential NaNs
            unique_values = df[col].dropna().unique().tolist()
            if unique_values:
                print(f"Creating Dropdown for '{col}' with values: {unique_values[:10]}...") # Print a sample
                categorical_inputs[col] = gr.Dropdown(unique_values, label=col)
            else:
                 print(f"Creating Textbox for '{col}' (no unique values found or all NaN)...")
                 categorical_inputs[col] = gr.Textbox(label=col)

        else:
             print(f"Warning: Categorical column '{col}' not found in the original DataFrame `df`.")
             # Fallback to Textbox if the column isn't in the original df
             categorical_inputs[col] = gr.Textbox(label=col)
else:
    print("\nNo categorical columns found or `df` not available. Using default Textbox inputs.")
    # Define some default categorical inputs if `categorical_cols` or `df` are not available
    # This is a fallback and might need manual adjustment
    categorical_inputs = {
        'State': gr.Textbox(label="State"), # Example fallback
        # Add other expected categorical features here as Textboxes if needed
    }


# Define numerical inputs based on expected features
# This assumes you know the names of the numerical features used.
# Example numerical features: 'Unemployment Rate', 'Poverty Rate', 'Crime Rate'
numerical_inputs = {
    'Unemployment Rate': gr.Number(label="Unemployment Rate", value=5.0),
    'Poverty Rate': gr.Number(label="Poverty Rate", value=10.0),
    'Crime Rate': gr.Number(label="Crime Rate", value=300.0),
    # Add other expected numerical features here
    # 'Another Numerical Feature': gr.Number(label="Another Numerical Feature", value=...)
}

# Combine all input components
# The order here doesn't strictly matter for the function, but it defines the UI layout.
input_components = []
input_components.extend(numerical_inputs.values())
input_components.extend(categorical_inputs.values())


# Define the function to be wrapped by Gradio
# This function will take the values from the Gradio inputs,
# format them into a DataFrame, and call the `preprocess_and_predict` function.

def predict_churn_gradio(*input_values):
    """
    Gradio interface function to take input values, preprocess, and predict churn.

    Args:
        *input_values: Variable number of arguments corresponding to the values
                       from the Gradio input components, in the order they are
                       passed to the interface.

    Returns:
        str: A string message containing the prediction result and interpretation.
    """
    # Create a dictionary from input component names and values
    # We need to map the input_values back to their feature names.
    # The order of `input_values` matches the order of `input_components`
    # defined above.

    feature_names = list(numerical_inputs.keys()) + list(categorical_inputs.keys())

    if len(input_values) != len(feature_names):
        return "Error: Mismatch between expected features and provided inputs."

    # Create a dictionary for the new data sample
    new_data_sample = dict(zip(feature_names, input_values))

    # Convert scalar values (from Number/Dropdown) into lists of length 1
    # to match the expected DataFrame format of `preprocess_and_predict`
    new_data_list = {k: [v] for k, v in new_data_sample.items()}

    # Create a DataFrame from the new data sample
    new_input_df = pd.DataFrame(new_data_list)

    print("\nReceived new input via Gradio:")
    print(new_input_df)

    # Ensure required objects from previous steps are available
    if 'model' in locals() and 'scaler' in locals() and 'X' in locals() and 'categorical_cols' in locals():
        # Need the column names of X (features after encoding, before scaling)
        training_columns_after_encoding = X.columns.tolist()

        # Call the preprocessing and prediction function
        try:
            predicted_class, predicted_prob, result_interpretation = preprocess_and_predict(
                new_input_df,
                model,
                scaler,
                training_columns_after_encoding,
                categorical_cols
            )

            # Format the output message
            output_message = f"Predicted Class (0: No Churn, 1: Churn): {predicted_class[0]}<br>"
            output_message += f"Prediction Probability (0: No Churn, 1: Churn): {predicted_prob[0][0]:.4f} (No Churn), {predicted_prob[0][1]:.4f} (Churn)<br>"
            output_message += f"Interpretation: {result_interpretation}"

            return output_message

        except Exception as e:
            return f"An error occurred during prediction: {e}"

    else:
        return "Error: Model or scaler not found. Please run the model training steps."


# Define the output component for the Gradio app
output_component = gr.HTML(label="Prediction Result") # Use HTML to display formatted output

# Build the Gradio interface
if len(input_components) > 0 and 'predict_churn_gradio' in locals():
    print("\nBuilding Gradio Interface...")
    iface = gr.Interface(
        fn=predict_churn_gradio,
        inputs=input_components,
        outputs=output_component,
        title="Churn Prediction App",
        description="Enter socioeconomic and crime data to predict churn."
    )

    # Launch the interface
    print("\nLaunching Gradio Interface...")
    iface.launch(debug=True) # Use debug=True for helpful debugging information
else:
    print("\n‚ùå Error: Could not build Gradio interface.")
    print("Ensure input_components are defined and predict_churn_gradio function exists.")
    print("This might happen if required variables (df, categorical_cols, model, scaler, X) were not defined in previous steps.")